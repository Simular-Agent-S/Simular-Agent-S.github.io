<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="Agent S: An Open Agentic Framework that Uses Computers Like a Human"/>
  <meta property="og:description" content="Agent S: An Open Agentic Framework that Uses Computers Like a Human"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="static/image/your_banner_image.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>Agent S: An Open Agentic Framework that Uses Computers Like a Human</title>
  <!-- <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <!-- <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet"> -->

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <script src="https://kit.fontawesome.com/deb78776bf.js" crossorigin="anonymous"></script>
</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-2 publication-title">Agent S: An Open Agentic Framework that Uses Computers Like a Human</h1>
            <br>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://www.simular.ai/" target="_blank">Simular AI</a></span>
              <!-- <span class="author-block">
                <a href="https://jiuzhouh.github.io" target="_blank">Jiuzhou Han</a><sup>1</sup>,</span>
                <span class="author-block">
                <a href="https://bayesian-models.org" target="_blank">Wray Buntine</a><sup>2</sup>,</span>
                <span class="author-block">
                <a href="https://eehsan.github.io" target="_blank">Ehsan Shareghi</a><sup>1</sup>,</span>
                </span> -->
            </div>

                  <!-- <div class="is-size-5 publication-authors">
                    <span class="author-block">Department of Data Science & AI, Monash University<sup>1</sup>, <br> College of Engineering and Computer Science, VinUniversity<sup>2</sup></span>
                  </div> -->

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary Model link -->
                    <!-- <span class="link-block">
                      <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa-regular fa-box-archive"></i>
                      </span>
                      <span>Model</span>
                    </a>
                  </span> -->

                    <!-- Supplementary data link -->
                    <!-- <span class="link-block">
                      <a href="" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fa fa-database"></i>
                      </span>
                      <span>Dataset</span>
                    </a>
                  </span> -->

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>

                <!-- ArXiv abstract Link
                <span class="link-block">
                  <a href="https://arxiv.org/abs/<ARXIV PAPER ID>" target="_blank"
                  class="external-link button is-normal is-rounded is-dark">
                  <span class="icon">
                    <i class="ai ai-arxiv"></i>
                  </span>
                  <span>arXiv</span>
                </a>
              </span> -->
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="item has-text-centered">
      <img src="static/videos/teaser.m4v" type="video/m4v" alt="" class="teaser-video" width="100%" muted="true" preload ="auto">
      </div>
      <br>
    </div>
  </div>
</section>

<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <div class="item has-text-centered">
      <img src="static/figures/teaser_final.png" alt="Teaser image" class="teaser-image" width="100%">
      </div>
      <br>
      <h2 class="subtitle has-text-justified">
        Agent S is a new agentic framework designed to enable computers to be used as intuitively as a human would. We introduce an Experience-Augmented Hierarchical Planning method. This method utilizes Online Web Knowledge for up-to-date information on frequently changing software and websites, along with Narrative Memory to leverage high-level experiences from past interactions. By breaking complex tasks into manageable subtasks and using Episodic Memory for step-by-step guidance, Agent S continuously refines its actions and learns from experience, achieving adaptable and effective task planning.
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>  We present Agent S, an open agentic framework that enables autonomous interaction with computers through Graphical User Interface (GUI), aimed at transforming human-computer interaction by automating complex, multi-step tasks. Agent S addresses three key challenges in automating computer tasks: acquiring domain-specific knowledge, planning over long task horizons, and handling dynamic, non-uniform interfaces. To this end, Agent S introduces experience-augmented hierarchical planning, which learns from external knowledge search and internal experience retrieval at multiple levels, facilitating efficient task planning and subtask execution. In addition, it employs an Agent-Computer Interface to better elicit the reasoning and control capabilities of GUI agents based on Multimodal Large Language Models. Evaluation on the OSWorld benchmark shows that Agent S outperforms the baseline by 9.37% on success rate (an 83.6% relative improvement) and achieves a new state-of-the-art. Comprehensive analysis highlights the effectiveness of individual components and provides insights for future improvements. Furthermore, Agent S demonstrates broad generalizability to different operating systems on a newly-released WindowsAgentArena benchmark.
          </p>
      </div>
    </div>
  </div>
</section>


<!-- End paper abstract -->
<div class="container is-max-desktop">
  <div class="item has-text-centered">
    <img src="static/figures/tb_example.png" alt="Teaser image" class="teaser-image" width="100%">
    </div>
<!-- <div class="columns is-centered">
  <div style="width: 100%;">
    <div class="item has-text-centered">
      <caption> The ablation study of ACI in OSWorld \(test_{sub}\).<br></caption>
      <img src="static/figures/thunderbird_example/step_0.png" alt="Teaser image" class="teaser-image" width="90%">
    </div>
  </div>
  <div style="width: 100%;">
    <div class="item has-text-centered">
      <caption> The ablation study of Memory Update in OSWorld \(test_{sub}\).<br></caption>
      <img src="static/figures/thunderbird_example/step_1.png" alt="Teaser image" class="teaser-image" width="90%">
    </div>
  </div>
  <div style="width: 100%;">
    <div class="item has-text-centered">
      <caption> The ablation study of ACI in OSWorld \(test_{sub}\).<br></caption>
      <img src="static/figures/thunderbird_example/step_2.png" alt="Teaser image" class="teaser-image" width="90%">
    </div>
  </div>
</div>
</div>
<div class="container is-max-desktop">
<div class="columns is-centered">
  <div style="width: 100%;">
    <div class="item has-text-centered">
      <caption> The ablation study of ACI in OSWorld \(test_{sub}\).<br></caption>
      <img src="static/figures/thunderbird_example/step_3.png" alt="Teaser image" class="teaser-image" width="90%">
    </div>
  </div>
  <div style="width: 100%;">
    <div class="item has-text-centered">
      <caption> The ablation study of Memory Update in OSWorld \(test_{sub}\).<br></caption>
      <img src="static/figures/thunderbird_example/step_4.png" alt="Teaser image" class="teaser-image" width="90%">
    </div>
  </div>
  <div style="width: 100%;">
    <div class="item has-text-centered">
      <caption> The ablation study of ACI in OSWorld \(test_{sub}\).<br></caption>
      <img src="static/figures/thunderbird_example/step_5.png" alt="Teaser image" class="teaser-image" width="90%">
    </div>
  </div>
</div> -->
</div>

<!-- Trajectories -->
<section class="section" id="">
  <div class="container is-max-desktop content">
    <h2 class="title">Overview of Agent S Framework</h2>
    <div class="item has-text-centered">
      <img src="static/figures/framework.png" alt="Teaser image" class="teaser-image" width="100%">
      </div>
      <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
      <script id="MathJax-script" async src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
      <p> Given task \( T_u \) and initial environment observation \( o_0 \), the Manager conducts experience-augmented hierarchical planning using web knowledge and narrative memory to produce subtasks \( s_0,\dotsc,s_n \). For each \( s_i \), Worker \( w_i \) draws from episodic memory to generate an action \( a_t \) at time \( t \), which is executed by the ACI to return the next immediate observation \( o_{t+1} \). A self-evaluation module closes the loop by storing the summarized subtask and full-task trajectories in narrative and episodic memory.
      </p>
  </div>
</section>

<!--Result-->
<section class="section" id="">
  <div class="container is-max-desktop content">
    <h2 class="title">Pipeline of Memory Construction and Update</h2>
    <div class="item has-text-centered">
      <img src="static/figures/learning_new.png" alt="Teaser image" class="teaser-image" width="90%">
      </div>
      <p>The pipeline of memory construction and update, which contains two phases: Self-supervised Exploration and Continual Memory Update. The initial Narrative & Episodic Memory is constructed through some randomly curated tasks during the exploration phase, and then it is updated based on the inference tasks continually.</p>
    </div>
  </section>

<section class="section" id="">
  <div class="container is-max-desktop content">
    <h2 class="title">Main Result</h2>
      <div class="item has-text-centered">
        <caption> Main results of Successful Rate (%) on the OSWorld full test set of all 369 test examples.<br></caption>
        <img src="static/figures/main_result.png" alt="Teaser image" class="teaser-image" width="70%">
        </div>
<p>This table shows the performance comparison between Agent S and the baseline models, evaluated across the whole OSWorld test set. For the GPT-4o model, Agent S achieves an overall success rate of 20.58%, nearly doubling the performance of the best corresponding baseline (GPT-4o with 11.21%). Agent S consistently outperforms the baselines in the “Daily” and “Professional” tasks, where it reaches 27.06% and 36.73% success rates, respectively, compared to the best baseline results of 12.33% and 14.29%. These tasks are commonly used in daily life or involved with knowledge-intensive professional applications, which benefit more from the retrieval augmentation of Agent S. Both Claude-3.5-Sonnet and GPT-4o outperform the baseline versions across the majority of tasks. Claude-3.5-Sonnet even performs better than GPT-4o in “Daily” and “Professional” tasks. The results demonstrate the enhanced capability of Agent S in handling diverse and complex tasks more effectively than the baseline approaches. </p>

</div>
</section>

<!-- Example -->
<section class="section" id="example">
  <div class="container is-max-desktop content">
    <h2 class="title">Analysis</h2>
    <p>To demonstrate the effectiveness of individual modules of Agent S, we stratified sampled a subset of 65 instances, \(test_{sub}\) from the full test set for the ablation study. Considering the inference cost, we utilized GPT-4o as the LLM backbone for all ablation studies for both the baseline and Agent S.</p>
    <h4 class="title">Learning from experience enhances the domain knowledge of GUI agents.</h4>
    <div class="item has-text-centered">
      <caption> The ablation study of experience-augmented hierarchical planning in OSWorld \(test_{sub}\). The metric is Successful Rate (%).<br></caption>
      <img src="static/figures/rag_ablation.png" alt="Teaser image" class="teaser-image" width="90%">
      </div>
      <p>Learning from universal experience available as web knowledge allows Agent S to make informed plans across a wide range of tasks and has the most significant impact.
The learning from Narrative and Episodic memories synergies effectively with web retrieval, and the results detail how their ablation affects the agent’s ability to handle complex tasks, underscoring the value of experiential learning. These results demonstrate that each component plays a critical role in enhancing the agent’s domain knowledge. Removing all three components (w/o All) degrades the performance significantly, revealing the importance of <i>learning from experience</i> in the design. </p>

<br>
    <!-- insert two figures data_scale and data_type side by side-->
    <div class="columns is-centered">
      <div style="width: 50%;">
        <div class="item has-text-centered">
          <caption> The ablation study of ACI in OSWorld \(test_{sub}\).<br></caption>
          <img src="static/figures/aci_ablation_slim.png" alt="Teaser image" class="teaser-image" width="90%">
        </div>
      </div>
      <div style="width: 50%;">
        <div class="item has-text-centered">
          <caption> The ablation study of Memory Update in OSWorld \(test_{sub}\).<br></caption>
          <img src="static/figures/memory_ablation_slim.png" alt="Teaser image" class="teaser-image" width="90%">
        </div>
      </div>
  </div>


<h4 class="title">ACI elicits better reasoning abilities of LLMs and supports better agentic learning.</h4>
<p> Comparing the baseline with <i>Agent S (ACI-only)</i> highlights the enhanced reasoning abilities achieved by incorporating ACI. Additionally, we examined the impact of ACI on agentic learning by integrating the Experiential learning process. For the baseline, adding Experiential learning slightly improved overall performance. However, when added to <i>Agent S (ACI-only)</i>, the performance improved significantly, demonstrating ACI's effectiveness in enhancing agentic learning. </p>

<h4 class="title">Hierarchical Planning supports long-horizon workflows.</h4>
<p>The <i>ACI-only + Experiential Learning </i> setup in shows Agent S performance without Hierarchical Planning, and the observed performance drop (26.15% to 20.00%) compared to the full Agent S underscores the importance of Hierarchical Planning in modeling long-horizon workflows. The effect of hierarchical formulation becomes pronounced in the presence of Experiential learning as the Manager can generate more detailed and accurate plans in the subtask planning stage.</p>

  <h4 class="title">Exploration, Continual Memory Update and Self-Evaluator are indispensable for memory construction.</h4>
  <p> Removing exploration limits memory updates to the inference phase only. Removing the continual memory update means we only use the memory obtained from the exploration phase without subsequent updates. Removing the self-evaluator involves replacing summarized experiences with the original full trajectories. The results reveal that ablating both the continual memory update and self-supervised exploration phases results in a performance drop, with the self-supervised exploration being much more impactful. The ablation of the Self-Evaluator further shows the benefits of using summarized trajectories instead of full trajectory exemplars for planning. </p>


  <!-- <table class="tg">

  <thead>
    <tr>
      <th></th>
      <th>Standard</th>
      <th>CoT</th>
      <th>ReAct</th>
      <th>UALA-S</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>ChatGPT</td>
      <td>0.5s/it</td>
      <td>1s/it</td>
      <td>12s/it</td>
      <td>3s/it</td>
    </tr>
    <tr>
      <td>LLaMA2-70B</td>
      <td>50s/it</td>
      <td>50s/it</td>
      <td>180s/it</td>
      <td>70s/it</td>
    </tr>
    <tr>
      <td>LLaMA2-13B</td>
      <td>25s/it</td>
      <td>25s/it</td>
      <td>120s/it</td>
      <td>45s/it</td>
    </tr>
    <tr>
      <td>LLaMA2-7B</td>
      <td>20s/it</td>
      <td>20s/it</td>
      <td>100s/it</td>
      <td>35s/it</td>
    </tr>
  </tr>
  <tr>
  </tbody>
  <caption>The average inference time per instance (seconds/iteration) of methods for HotpotQA. LLaMA2 inference uses a single A40.<br></caption>
  </table>

  <p>Standard and CoT prompting methods do not involve an external tool call, hence faster inference time compared to other methods. As indicated, UALA-S given its selective tool call, has a much lower inference time compared with ReAct. This highlights a practical benefit of using uncertainty to reduce the number of token usage and tool calls, while still providing a significant gain in performance.</p>
<h3 class="title">Answer Uncertainty Visualisation</h3>
  <div class="item has-text-centered">
    <caption> The visualisation (boxplot) of uncertainty range for correct and incorrect answers of three datasets on ChatGPT. <br></caption>
    <img src="static/figures/rag_ablation.png" alt="Teaser image" class="teaser-image" width="90%">
  </div>

  <p>In both single-inference and multi-inference settings, correct answers consistently exhibit lower uncertainty compared to incorrect ones. This difference is statistically significant. When calculating the difference between the average uncertainty of correct and incorrect answers we observe the largest difference to belong to HotpotQA, followed by StrategyQA, and MMLU. This explains why the gain from UALA follows the same pattern in the main results.</p> -->

  <!-- <table class="tg">
  <thead>
    <tr>
      <th>Methods</th>
      <th>EM</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CoT</td>
      <td>34.8(0)</td>
    </tr>
    <tr>
      <td>CRITIC</td>
      <td>41.0(1,500)</td>
    </tr>
    <tr>
      <td>CRITIC w/o Tool</td>
      <td>35.6(1,500)</td>
    </tr>
    <tr>
      <td>UALA-S-CRITIC</td>
      <td>39.0(597)</td>
    </tr>
    <tr>
      <td>UALA-S-CRITIC w/o Tool</td>
      <td>38.0(597)</td>
    </tr>
    <tr>
      <td>UALA-M-CRITIC</td>
      <td>40.6(795)</td>
    </tr>
    <tr>
      <td>UALA-M-CRITIC w/o Tool</td>
      <td>37.4(795)</td>
    </tr>
  </tbody> -->
  <!-- </table> -->

<!-- <h3 class="title">Language Agent Fine-tuning vs. UALA</h3>
    <table class="tg">
    <thead>
      <tr>
        <th>Tasks</th>
        <th>Methods</th>
        <th>Training Size</th>
        <th>ChatGPT</th>
        <th>LLaMA2-70B</th>
      </tr>
    </thead>
    <tbody>
      <tr>
        <td rowspan="4">HotpotQA</td>
        <td>FireAct</td>
        <td>162</td>
        <td>27.8</td>
        <td>27.8</td>
      </tr>
      <tr>
        <td>FireAct</td>
        <td>512</td>
        <td>33.8</td>
        <td>30.0</td>
      </tr>
      <tr>
        <td>ReAct</td>
        <td>No fine-tuning</td>
        <td>32.0</td>
        <td>32.4</td>
      </tr>
      <tr>
        <td>UALA-S</td>
        <td>No fine-tuning</td>
        <td><b>38.2</b></td>
        <td><b>36.4</b></td>
      </tr>
      <tr>
        <td rowspan="4">StrategyQA</td>
        <td>FireAct</td>
        <td>283</td>
        <td>60.7</td>
        <td>63.8</td>
      </tr>
      <tr>
        <td>FireAct</td>
        <td>567</td>
        <td>64.9</td>
        <td>64.6</td>
      </tr>
      <tr>
        <td>ReAct</td>
        <td>No fine-tuning</td>
        <td>55.5</td>
        <td>58.1</td>
      </tr>
      <tr>
        <td>UALA-S</td>
        <td>No fine-tuning</td>
        <td><b>65.6</b></td>
        <td><b>69.0</b></td>
      </tr>
    </tr>
    <tr>
    </tbody>
    <caption>Results of FireAct vs. UALA-S. The ReAct and UALA-S results are based on 6-shot and the off-the-shelf LLM backbones.<br></caption>
    </table>
        <p>We demonstrate the comparison between UALA-S and fine-tuning language agents following the FireAct setting. For ChatGPT, we use the official GPT-3.5-Turbo fine-tuning API; for LLaMA2-70B, we use LoRA. To have a side-by-side comparison, we use the same 500 training samples used for the calibration set, to construct the fine-tuning data. Mimicking the FireAct setting, we ran the 500 examples using ReAct with ChatGPT, and collected the successful trajectories as the training data for FireAct. This amounted to 162 training examples for HotpotQA and 283 for StrategyQA. In addition, to match the amount of training data as FireAct setting, we also ran an additional 1000 examples to increase the amount of successful training trajectories to 512 for HotpotQA and 567 for StrategyQA.  </p>
          <p>Interestingly, on HotpotQA using 162 training examples, FireAct under-performs the few-shot (6-shots) ReAct agent, while it outperforms ReAct on StrategyQA using 283 training examples. Increasing the amount of training data to 500+ leads to improvement on both LLMs with fine-tuned ChatGPT-based agents outperforming the ReAct counterpart on both datasets. Our method, UALA-S, achieves the best result without any fine-tuning and using only the 500 samples for creating the calibration set. This capitalises an obvious empirical advantage for utilising uncertainty instead of fine-tuning in the presence of small amount of data.</p>
      </div> -->
    </div>
</section>

<section class="section" id="">
  <div class="container is-max-desktop content">
    <h2 class="title">Generalization to Different Operating Systems </h2>
      <div class="item has-text-centered">
        <caption> Results of Successful Rate (%) on WindowsAgentArena using GPT-4o and Image + Accessibility Tree input on the full test set of all 154 test examples.<br></caption>
        <img src="static/figures/windows_result.png" alt="Teaser image" class="teaser-image" width="90%">
        </div>
<p>We test the Agent S framework with no modification on WindowsAgentArena, a Windows OS benchmark released contemporaneously with our work. We compare Agent S with the similar configuration with GPT-4o as the MLLM backbone, Accessibility Tree + Image as the input, and parsing with OCR. As shown in the table, Agent S outperforms the Navi agent without any adaptation to the new Windows environment.</p>

</div>
</section>


<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>@misc{AgentS,
      title={Agent S: An Open Agentic Framework that Uses Computers Like a Human},
      author={Saaket Agashe<sup>*</sup>, Jiuzhou Han<sup>*</sup>, Shuyu Gan, Jiachen Yang, Ang Li, Xin Eric Wang},
      year={2024},
      eprint={},
      archivePrefix={arXiv},
      primaryClass={cs.CL}
}</code></pre>
    </div>
</section>
<!--End BibTex citation -->




  <!-- <footer class="footer">
    <div class="container is-max-desktop content">
    <div style="display: flex; justify-content: space-between;">
      <div style="width: 30%; margin-left: 0px; margin-right: 0px;">
        <a href="https://www.monash.edu/" target="_blank">
          <img src="static/images/monash-logo.png" alt="Monash University" width="100%">
        </a>
      </div>
      <div style="width: 40%;">
        <a href="https://vinuni.edu.vn" target="_blank">
          <img src="static/images/vin-logo.svg" alt="Vin University" width="100%">
        </a>
      </div>
    </div>
  </div>
</footer> -->

<!-- Statcounter tracking code -->

<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
